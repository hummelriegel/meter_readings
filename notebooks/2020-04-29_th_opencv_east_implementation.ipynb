{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Implementation of OpenCV\n",
    "Following https://nanonets.com/blog/deep-learning-ocr/ implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the necessary packages \n",
    "import numpy as np\n",
    "import cv2\n",
    "from imutils.object_detection import non_max_suppression\n",
    "#import pytesseract\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV:\n",
    "* OpenCV Model Overviews: https://github.com/opencv/opencv_extra\n",
    "* Download Model: https://github.com/ZER-0-NE/EAST-Detector-for-text-detection-using-OpenCV/blob/master/frozen_east_text_detection.pb\n",
    "* Code for Self-Training: https://github.com/argman/EAST (maybe tensorflow slim resnet_v1_50.ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating argument dictionary for the default arguments needed in the code. \n",
    "\n",
    "image_path = os.path.join(os.pardir, 'data', 'raw', '2017', '2017-03-03 15.06.47.jpg')\n",
    "model_path = os.path.join(os.pardir, 'models', 'frozen_east_text_detection.pb')\n",
    "# model_path = os.path.join(os.pardir, 'models', 'resnet_v1_50.ckpt')\n",
    "\n",
    "#args = {\"image\":'../data/raw/2017/2017-03-03 15.06.47.jpg',\n",
    "args = {\"image\":image_path,\n",
    "        \"east\":model_path,\n",
    "        \"min_confidence\":0.5,\n",
    "        \"width\":640, #multiple of 32\n",
    "        \"height\":640} #multiple of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give location of the image to be read.\n",
    "#\"Example-images/ex24.jpg\" image is being loaded here. \n",
    "\n",
    "#args['image']=\"../input/text-detection/example-images/Example-images/ex24.jpg\"\n",
    "image = cv2.imread(args['image'])\n",
    "\n",
    "#Saving a original image and shape\n",
    "orig = image.copy()\n",
    "(origH, origW) = image.shape[:2]\n",
    "\n",
    "# set the new height and width to default 320 by using args #dictionary.  \n",
    "(newW, newH) = (args[\"width\"], args[\"height\"])\n",
    "\n",
    "#Calculate the ratio between original and new image for both height and weight. \n",
    "#This ratio will be used to translate bounding box location on the original image. \n",
    "rW = origW / float(newW)\n",
    "rH = origH / float(newH)\n",
    "\n",
    "# resize the original image to new dimensions\n",
    "image = cv2.resize(image, (newW, newH))\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# construct a blob from the image to forward pass it to EAST model\n",
    "# Parameter-Explanations: https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga29f34df9376379a603acd8df581ac8d7\n",
    "blob = cv2.dnn.blobFromImage(image,\n",
    "                             1.0,\n",
    "                             (W, H),\n",
    "                             (123.68, 116.78, 103.94),\n",
    "                             swapRB=False,\n",
    "                             crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test show image\n",
    "from PIL import Image\n",
    "pil_im = Image.open(image_path, 'r')\n",
    "pil_im.show()\n",
    "#display(pil_im)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![Test](../data/raw/2017/2017-03-03%2015.06.47.jpg \"Bild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new image\n",
    "print(\"blob.shape\", blob.shape)\n",
    "print(\"blob[0,:].shape\", blob[0,:].shape)\n",
    "print(\"np.moveaxis(blob[0,:],0,-1).shape\", np.moveaxis(blob[0,:],0,-1).shape)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(np.moveaxis(blob[0,:],0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained EAST model for text detection\n",
    "net = cv2.dnn.readNet(args[\"east\"])\n",
    "\n",
    "# We would like to get two outputs from the EAST model. \n",
    "#1. Probabilty scores for the region whether that contains text or not. \n",
    "#2. Geometry of the text -- Coordinates of the bounding box detecting a text\n",
    "# The following two layer need to pulled from EAST model for achieving this. \n",
    "layerNames = [\n",
    "    \"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \"feature_fusion/concat_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward pass the blob from the image to get the desired output layers\n",
    "net.setInput(blob)\n",
    "(scores, geometry) = net.forward(layerNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"scores.shape\", scores.shape)\n",
    "#print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns a bounding box and probability score if it is more than minimum confidence\n",
    "def predictions(prob_score, geo):\n",
    "    (numR, numC) = prob_score.shape[2:4]\n",
    "    boxes = []\n",
    "    confidence_val = []\n",
    "\n",
    "    # loop over rows\n",
    "    for y in range(0, numR):\n",
    "        scoresData = prob_score[0, 0, y]\n",
    "        x0 = geo[0, 0, y]\n",
    "        x1 = geo[0, 1, y]\n",
    "        x2 = geo[0, 2, y]\n",
    "        x3 = geo[0, 3, y]\n",
    "        anglesData = geo[0, 4, y]\n",
    "\n",
    "        # loop over the number of columns\n",
    "        for i in range(0, numC):\n",
    "            if scoresData[i] < args[\"min_confidence\"]:\n",
    "                continue\n",
    "\n",
    "            (offX, offY) = (i * 4.0, y * 4.0)\n",
    "\n",
    "            # extracting the rotation angle for the prediction and computing the sine and cosine\n",
    "            angle = anglesData[i]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "\n",
    "            # using the geo volume to get the dimensions of the bounding box\n",
    "            h = x0[i] + x2[i]\n",
    "            w = x1[i] + x3[i]\n",
    "\n",
    "            # compute start and end for the text pred bbox\n",
    "            endX = int(offX + (cos * x1[i]) + (sin * x2[i]))\n",
    "            endY = int(offY - (sin * x1[i]) + (cos * x2[i]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "\n",
    "            boxes.append((startX, startY, endX, endY))\n",
    "            confidence_val.append(scoresData[i])\n",
    "\n",
    "    # return bounding boxes and associated confidence_val\n",
    "    return (boxes, confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find predictions and  apply non-maxima suppression\n",
    "# https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/\n",
    "(boxes, confidence_val) = predictions(scores, geometry)\n",
    "boxes = non_max_suppression(np.array(boxes), probs=confidence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Text Detection and Recognition \n",
    "\n",
    "# initialize the list of results\n",
    "results = []\n",
    "\n",
    "# loop over the bounding boxes to find the coordinate of bounding boxes\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "    # scale the coordinates based on the respective ratios in order to reflect bounding box on the original image\n",
    "    startX = int(startX * rW)\n",
    "    startY = int(startY * rH)\n",
    "    endX = int(endX * rW)\n",
    "    endY = int(endY * rH)\n",
    "\n",
    "    #extract the region of interest\n",
    "    r = orig[startY:endY, startX:endX]\n",
    "\n",
    "    #configuration setting to convert image to string.  \n",
    "    #configuration = (\"-l eng --oem 1 --psm 8\")\n",
    "    ##This will recognize the text from the image of bounding box\n",
    "    #text = pytesseract.image_to_string(r, config=configuration)\n",
    "    text = \"\"\n",
    "\n",
    "    # append bbox coordinate and associated text to the list of results \n",
    "    results.append(((startX, startY, endX, endY), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the image with bounding box and recognized text\n",
    "orig_image = orig.copy()\n",
    "\n",
    "# Moving over the results and display on the image\n",
    "for ((start_X, start_Y, end_X, end_Y), text) in results:\n",
    "  # display the text detected by Tesseract\n",
    "  print(\"{}\\n\".format(text))\n",
    "\n",
    "  # Displaying text\n",
    "  text = \"\".join([x if ord(x) < 128 else \"\" for x in text]).strip()\n",
    "  cv2.rectangle(orig_image, (start_X, start_Y), (end_X, end_Y), (0, 0, 255), 2)\n",
    "  cv2.putText(orig_image, text, (start_X, start_Y - 30),\n",
    "  cv2.FONT_HERSHEY_SIMPLEX, 0.7,(0,0, 255), 2)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(orig_image)\n",
    "plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meter_readings",
   "language": "python",
   "name": "meter_readings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
